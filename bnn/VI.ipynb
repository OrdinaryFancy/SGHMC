{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7eaf093d",
   "metadata": {},
   "source": [
    "# Variational Inference and SGHMC\n",
    "\n",
    "The purpose of this notebook is to compare Variational Inference (VI) and Stochastic Gradient Hamiltonian Monte Carlo (SGHMC) is the MNIST Bayesian neural network (BNN) setting.\n",
    "\n",
    "As a fair comparison we allow (VI) 50 epochs to learn the variational parameters of the BNN, we also give SGHMC 50 burn-in epochs so that it has hopefully converged to the posterior distribution by this point (or at least found some good parameters). We then sample 80000 samples from the variational posterior and we (approximately) sample 80000 parameters from the true posterior using SGHMC.\n",
    "\n",
    "We the compare the two sample sets using a 50000/10000 train/validation split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01704b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import seaborn as sns # conda install seaborn\n",
    "import pandas as pd # ^^ this will automatically install pandas\n",
    "\n",
    "import pyro\n",
    "from pyro.infer.mcmc import MCMC\n",
    "import pyro.distributions as dist\n",
    "\n",
    "from kernel.sghmc import SGHMC\n",
    "\n",
    "from pyro.infer import SVI, Trace_ELBO, RenyiELBO\n",
    "from pyro.optim import SGD, Adam, ClippedAdam\n",
    "\n",
    "pyro.set_rng_seed(101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b3f0e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple dataset wrapper class\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        ### Hyperparams\n",
    "    def __len__(self):\n",
    "        return(len(self.data))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9f8ce5",
   "metadata": {},
   "source": [
    "### Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4d948c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 500\n",
    "NUM_EPOCHS = 800\n",
    "WARMUP_EPOCHS = 50\n",
    "HIDDEN_SIZE = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5270ae25",
   "metadata": {},
   "source": [
    "### Download MNIST and setup datasets / dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16f235a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST('./data', train=True, download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST('./data', train=False, download=True)\n",
    "\n",
    "nvalid = 10000\n",
    "### Download MNIST and setup datasets / dataloaders\n",
    "perm = torch.arange(len(train_dataset))\n",
    "train_idx = perm[nvalid:]\n",
    "val_idx = perm[:nvalid]\n",
    "    \n",
    "mean = 0.1307\n",
    "std = 0.3081\n",
    "\n",
    "# scale and normalise the datasets\n",
    "X_train = (train_dataset.data[train_idx] / 255.0 - mean ) / std\n",
    "Y_train = train_dataset.targets[train_idx]\n",
    "\n",
    "X_val = (train_dataset.data[val_idx] / 255.0 - mean) / std \n",
    "Y_val = train_dataset.targets[val_idx]\n",
    "\n",
    "X_test = (test_dataset.data / 255.0 - mean) / std\n",
    "Y_test = test_dataset.targets\n",
    "\n",
    "# redefine the datasets\n",
    "train_dataset = Dataset(X_train, Y_train)\n",
    "val_dataset = Dataset(X_val, Y_val)\n",
    "test_dataset = Dataset(X_test, Y_test)\n",
    "\n",
    "# setup the dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a60675f",
   "metadata": {},
   "source": [
    "### Define the Bayesian neural network  model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a296b9b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_22788/980977717.py, line 28)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\alexw\\AppData\\Local\\Temp/ipykernel_22788/980977717.py\"\u001b[1;36m, line \u001b[1;32m28\u001b[0m\n\u001b[1;33m    with pyro.plate(\"data\", x.shape[0]):\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "PyroLinear = pyro.nn.PyroModule[torch.nn.Linear]\n",
    "    \n",
    "class BNN(pyro.nn.PyroModule):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, prec=1.):\n",
    "        super().__init__()\n",
    "        # prec is a kwarg that should only used by SGD to set the regularization strength \n",
    "        # recall that a Guassian prior over the weights is equivalent to L2 norm regularization in the non-Bayes setting\n",
    "        \n",
    "        # TODO add gamma priors to precision terms\n",
    "        self.fc1 = PyroLinear(input_size, hidden_size)\n",
    "        self.fc1.weight = pyro.nn.PyroSample(dist.Normal(0., prec).expand([hidden_size, input_size]).to_event(2))\n",
    "        self.fc1.bias   = pyro.nn.PyroSample(dist.Normal(0., prec).expand([hidden_size]).to_event(1))\n",
    "        \n",
    "        self.fc2 = PyroLinear(hidden_size, output_size)\n",
    "        self.fc2.weight = pyro.nn.PyroSample(dist.Normal(0., prec).expand([output_size, hidden_size]).to_event(2))\n",
    "        self.fc2.bias   = pyro.nn.PyroSample(dist.Normal(0., prec).expand([output_size]).to_event(1))\n",
    "        \n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.log_softmax = torch.nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = self.log_softmax(### Define the Bayesian neural network  modelx)# output (log) softmax probabilities of each class\n",
    "        \n",
    "        with pyro.plate(\"data\", x.shape[0]):\n",
    "            obs = pyro.sample(\"obs\", dist.Categorical(logits=x), obs=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746a7b70",
   "metadata": {},
   "source": [
    "### Run VI\n",
    "\n",
    "We run VI using the RenyiELBO to get a tight variational bound, we use the ClippedAdam optimizer which implements weight decay, gradient clipping, and learning rate decay.\n",
    "\n",
    "We run this for 50 epochs but typically the validation loss diverges and so we stop early if the validation loss ever decreases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85120f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "\n",
    "bnn = BNN(28*28, HIDDEN_SIZE, 10)\n",
    "guide = pyro.infer.autoguide.AutoDiagonalNormal(bnn)\n",
    "\n",
    "svi = SVI(model=bnn, \n",
    "          guide=guide, \n",
    "          optim=ClippedAdam({\"lr\": 1e-3, \"weight_decay\": 1e-3, \"clip_norm\": 1.0}), \n",
    "          loss=RenyiELBO(alpha=0.1, num_particles=2))\n",
    "\n",
    "num_epochs = WARMUP_EPOCHS\n",
    "epoch = 0\n",
    "\n",
    "normalizer = len(train_loader.dataset)\n",
    "\n",
    "test_acc = []\n",
    "\n",
    "while epoch < num_epochs:\n",
    "    losses = []\n",
    "    for imgs, labels in train_loader:\n",
    "        total_loss = svi.step(imgs, labels)\n",
    "        loss = total_loss / normalizer\n",
    "        losses += [loss]\n",
    "        \n",
    "    epoch += 1\n",
    "        \n",
    "    print('Epoch {} avg loss {:.4f}'.format(epoch, np.mean(losses)))\n",
    "    \n",
    "    predictive = pyro.infer.Predictive(model=bnn, guide=guide, num_samples=100)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for imgs, labels in val_loader:\n",
    "            out = predictive(imgs)\n",
    "            pred = out['obs'].mode(0)[0]\n",
    "            total += labels.shape[0]\n",
    "            correct += int((pred == labels).sum())\n",
    "\n",
    "        print(\"test accuracy:\",correct/ total)\n",
    "        \n",
    "        if (correct / total) < test_acc[-1]:\n",
    "            break\n",
    "        else:\n",
    "            test_acc.append(correct/total)\n",
    "            continue\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c3231c",
   "metadata": {},
   "source": [
    "### Sample from the variational posterior\n",
    "\n",
    "We draw 80000 samples (100 at a time) from the variational posterior and record the validation error and accuracy using Bayesian averaging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee0f04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full posterior predictive \n",
    "full_predictive = torch.FloatTensor(10000, 10)\n",
    "full_predictive.zero_()\n",
    "\n",
    "vi_test_errs = []\n",
    "\n",
    "for epoch in range(1, 1+NUM_EPOCHS):\n",
    "    predictive = pyro.infer.Predictive(model=bnn, guide=guide, num_samples=100)\n",
    "    start = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        epoch_predictive = None\n",
    "        for x, y in val_loader:\n",
    "            if epoch_predictive is None:\n",
    "                epoch_predictive = predictive(x)['obs'].to(torch.int64)\n",
    "            else:\n",
    "                epoch_predictive = torch.cat((epoch_predictive, predictive(x)['obs'].to(torch.int64)), dim=1)\n",
    "                    \n",
    "        for sample in epoch_predictive:\n",
    "            predictive_one_hot = F.one_hot(sample, num_classes=10)\n",
    "            full_predictive = full_predictive + predictive_one_hot\n",
    "                \n",
    "        full_y_hat = torch.argmax(full_predictive, dim=1)\n",
    "        total = Y_val.shape[0]\n",
    "        correct = int((full_y_hat == Y_val).sum())\n",
    "            \n",
    "    end = time.time()\n",
    "        \n",
    "    vi_test_errs.append(1.0 - correct/total)\n",
    "\n",
    "    print(\"Epoch [{}/{}] test accuracy: {:.4f} time: {:.2f}\".format(epoch, NUM_EPOCHS, correct/total, end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8d5095",
   "metadata": {},
   "source": [
    "### Run SGHMC \n",
    "\n",
    "We run SGHMC to sample approximately from the posterior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3ad735",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 4e-6\n",
    "MOMENTUM_DECAY = 0.01\n",
    "RESAMPLE_EVERY_N = 100\n",
    "NUM_STEPS = 1\n",
    "\n",
    "pyro.clear_param_store()\n",
    "\n",
    "bnn = BNN(28*28, HIDDEN_SIZE, 10)\n",
    "\n",
    "sghmc = SGHMC(bnn,\n",
    "              subsample_positions=[0, 1],\n",
    "              batch_size=BATCH_SIZE,\n",
    "              learning_rate=LR,\n",
    "              momentum_decay=MOMENTUM_DECAY,\n",
    "              num_steps=NUM_STEPS,\n",
    "              resample_every_n=RESAMPLE_EVERY_N)\n",
    "\n",
    "sghmc_mcmc = MCMC(sghmc, num_samples=len(train_dataset)//BATCH_SIZE, warmup_steps=0)\n",
    "\n",
    "sghmc_test_errs = []\n",
    "\n",
    "# full posterior predictive \n",
    "full_predictive = torch.FloatTensor(10000, 10)\n",
    "full_predictive.zero_()\n",
    "\n",
    "for epoch in range(1, 1+NUM_EPOCHS + WARMUP_EPOCHS):\n",
    "    sghmc_mcmc.run(X_train, Y_train)\n",
    "    \n",
    "    if epoch >= WARMUP_EPOCHS:\n",
    "        \n",
    "        sghmc_samples = sghmc_mcmc.get_samples()\n",
    "        predictive = pyro.infer.Predictive(bnn, posterior_samples=sghmc_samples)\n",
    "        start = time.time()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            epoch_predictive = None\n",
    "            for x, y in val_loader:\n",
    "                if epoch_predictive is None:\n",
    "                    epoch_predictive = predictive(x)['obs'].to(torch.int64)\n",
    "                else:\n",
    "                    epoch_predictive = torch.cat((epoch_predictive, predictive(x)['obs'].to(torch.int64)), dim=1)\n",
    "                    \n",
    "            for sample in epoch_predictive:\n",
    "                predictive_one_hot = F.one_hot(sample, num_classes=10)\n",
    "                full_predictive = full_predictive + predictive_one_hot\n",
    "                \n",
    "            full_y_hat = torch.argmax(full_predictive, dim=1)\n",
    "            total = Y_val.shape[0]\n",
    "            correct = int((full_y_hat == Y_val).sum())\n",
    "            \n",
    "        end = time.time()\n",
    "        \n",
    "        sghmc_test_errs.append(1.0 - correct/total)\n",
    "\n",
    "        print(\"Epoch [{}/{}] test accuracy: {:.4f} time: {:.2f}\".format(epoch-WARMUP_EPOCHS, NUM_EPOCHS, correct/total, end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc69ed3",
   "metadata": {},
   "source": [
    "### Plot VI against SGHMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5f5f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"dark\")\n",
    "    \n",
    "sghmc_test_errs = np.array(sghmc_test_errs)\n",
    "vi_test_errs = np.array(vi_test_errs)\n",
    "\n",
    "err_dict = {'SGHMC' : sghmc_test_errs, 'VI' : vi_test_errs}\n",
    "x = np.arange(1, NUM_EPOCHS+1)\n",
    "lst = []\n",
    "for i in range(len(x)):\n",
    "    for updater in err_dict.keys():\n",
    "        lst.append([x[i], updater, err_dict[updater][i]])\n",
    "\n",
    "df = pd.DataFrame(lst, columns=['iterations', 'updater','test error'])\n",
    "sns.lineplot(data=df.pivot(\"iterations\", \"updater\", \"test error\"))\n",
    "plt.ylabel(\"test error\")\n",
    "plt.show() #dpi=300"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
