{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caabfcdf",
   "metadata": {},
   "source": [
    "# Demo\n",
    "\n",
    "This jupyter notebook contains a demo / introduction to using our implementation of Stochastic Gradient Hamiltionian Monte Carlo (SGHMC) which is built on top of Pyro. There are a few caveats that you should understand before trying to use our implementation, so this idea of this document is to help you to use your implementation correctly.\n",
    "\n",
    "This demo is very loosely inspired by: https://bookdown.org/robertness/causalml/docs/tutorial-on-deep-probabilitic-modeling-with-pyro.html#introduction-to-pyro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e062331",
   "metadata": {},
   "source": [
    "# Pyro\n",
    "\n",
    "Pyro is a universal probabilistic programming language (PPL) written in Python and supported by PyTorch on the backend. Pyro enables flexible and expressive deep probabilistic modeling, unifying the best of modern deep learning and Bayesian modeling.\n",
    "\n",
    "What this really means: Pyro provides a vast array of modelling and inference abilities, which are super-charged by PyTorch's automatic differentiation and GPU computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d26bc2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import pyro\n",
    "from pyro.infer.mcmc import MCMC\n",
    "import pyro.distributions as dist\n",
    "# our implementation of sghmc\n",
    "from sghmc import SGHMC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82f483b",
   "metadata": {},
   "source": [
    "# 1) Defining a model (aka, stochastic function)\n",
    "\n",
    "In Pyro, a model is defined using a **\"stochastic function\"**.\n",
    "\n",
    "A stochastic function is an arbitrary Python callable (function or method) that combines two ingredients:\n",
    "\n",
    "- Deterministic Python code\n",
    "- Primitive stochastic functions that call a random number generator.\n",
    "\n",
    "## Primitive stochastic functions\n",
    "\n",
    "These are basic elements which allow us to introduce stochasticity into our model, and essentially allow us to define different **distributions** in our model. The code below defines a normal distribution and samples from it:\n",
    "\n",
    "## Adding data to the model\n",
    "\n",
    "The key idea of SGHMC is that we subsample the observed data explained by the stochastic model and compute noisy gradient estimates which are used to simulate Hamiltonian dynamics in the same way as noise free HMC.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba4bec0",
   "metadata": {},
   "source": [
    "### A Simple model:\n",
    "\n",
    "This model is flexible it can take an arbitrary amount of data and condition on it to give us a posterior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e6e580e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(data):\n",
    "    weight = pyro.sample(\"weight\", dist.Normal(0, 1.0))\n",
    "    return pyro.sample(\"measurement\", dist.Normal(weight, 1.0), obs=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8192772",
   "metadata": {},
   "source": [
    "### Some data\n",
    "Below we create some data for our model to condition on. Note that any data passed to the model to be subsampled **must** first be wrapped in a pytorch tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0190ca0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor([8.5992, 6.4611, 6.7287, 4.2451, 5.5909, 5.4184, 7.3571, 6.3754, 8.0597,\n",
    "        8.7034])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0959e7e9",
   "metadata": {},
   "source": [
    "# 2) Inference\n",
    "\n",
    "## Inference using MCMC\n",
    "\n",
    "Now, all we need to do is use an inference algorithm to estimate the posterior distribution of the unobserved random variables in our model (in this case, the weight of the object), given the observed data.\n",
    "\n",
    "### Using SGHMC\n",
    "\n",
    "Our data is positional argument 0 of our model, this is the default behaviour, although if we want to subsample more than one positional arguments then we need to specify the positions in the `subsample_positions` argument.\n",
    "\n",
    "We can specify other things such as `batch_size`, `step_size`, `num_steps`, `with_friction`, `friction_term`, `friction_constant`, `obs_info_noise`, `do_mh_correction`, `do_step_size_adaptation`, `target_accept`\n",
    "\n",
    "Note that the naive SGHMC algorithm does **not** use friction (but we can turn Metropolis Hastings correction on for this configuration and see the difference in performance)\n",
    "\n",
    "Also note for the SGHMC algorithm we do **not** use MH correction since by using friction we get the desired invariant distribution. \n",
    "\n",
    "Its best not to set `do_step_size_adaptation = True` at the moment since the automatic step size adaptation is very unstable at the moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bca05cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|███████████████████████████████████████████████████████| 2000/2000 [00:12, 157.55it/s, step size=1.00e-01]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Stochastic Gradient Hamiltonian Monte Carlo kernel.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    subsample_positions : list, default [0] / 1st positional argument only\n",
    "        Specifies which positional arguments of the model to subsample during runtime\n",
    "        \n",
    "    batch_size : int, default=5\n",
    "        The size of the minibatches to use\n",
    "\n",
    "    step_size : int, default=1\n",
    "        The size of a single step taken while simulating Hamiltonian dynamics\n",
    "\n",
    "    num_steps : int, default 10\n",
    "        The number of steps to simulate Hamiltonian dynamics\n",
    "        \n",
    "    with_friction : bool, default False\n",
    "        Use friction term when updating momentum\n",
    "\n",
    "    do_mh_correction : bool, default False\n",
    "        compute the mh correction term using the whole dataset\n",
    "        \n",
    "    do_step_size_adaptation : bool, default True\n",
    "        Do step size adaptation during warm up phase\n",
    "    \"\"\"\n",
    "# Instantiate the SGHMC kernel\n",
    "sghmc_kernel = SGHMC(model, \n",
    "                     subsample_positions=[0],\n",
    "                     batch_size=5, \n",
    "                     step_size=0.1, \n",
    "                     num_steps=4, \n",
    "                     with_friction=True, \n",
    "                     friction_term=None, \n",
    "                     friction_constant=1.0,\n",
    "                     obs_info_noise=False,\n",
    "                     do_mh_correction=False,\n",
    "                     do_step_size_adaptation=False,\n",
    "                     target_accept=0.75)\n",
    "\n",
    "# Use pyros MCMC sampling module\n",
    "sghmc_mcmc = MCMC(sghmc_kernel, num_samples=1000)\n",
    "# Pass the model parameters when we call run\n",
    "sghmc_mcmc.run(data)\n",
    "# Fetch samples from the sampler\n",
    "sghmc_samples = sghmc_mcmc.get_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f214d9d2",
   "metadata": {},
   "source": [
    "### Plotting the data\n",
    "We can plot the empirical posterior distribution by plotting `sghmc_samples`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "332fec6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "samples = sghmc_samples['weight'].numpy()\n",
    "\n",
    "plt.hist(samples, density=True, bins=30)  # density=False would make counts\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b98c3ba",
   "metadata": {},
   "source": [
    "## More flexible models\n",
    "\n",
    "Our implementation of SGHMC is flexible enough to allow you to specify arbitrary positional and keyword arguments. Although you can **only** subsample positional arguments!!! This shouldn't be too restrictive of a requirement, but below we give good some examples of how to use our implementation of SGHMC properly and some bad examples.\n",
    "\n",
    "We won't run any inference but if you're interested but the following examples will be based on coin flipping using a beta prior and binomial likelihood. See `example.ipynb` and play around with it to convince yourself the following examples will work or not. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e4afb6",
   "metadata": {},
   "source": [
    "### Model with arbitrary key word arguments\n",
    "\n",
    "In this example we supply our model with hyper-priors for the beta distribution by using key word arguments. The default is `alpha0=1.` and `beta0=1.` which is just the uniform distribution on the unit interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd14d1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(data, alpha0=1., beta0=1.):\n",
    "    alpha0 = torch.tensor(alpha0)\n",
    "    beta0 = torch.tensor(beta0)\n",
    "    \n",
    "    f = pyro.sample(\"coin\", dist.Beta(alpha0, beta0))\n",
    "    \n",
    "    return pyro.sample(\"obs\", dist.Bernoulli(f), obs=dats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1dcdf9",
   "metadata": {},
   "source": [
    "If we want to run inference using our implementation of SGHMC we needn't change much we just need to specify `alpha0` and `beta0` by passing it to the `.run()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5203059f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the SGHMC kernel\n",
    "sghmc_kernel = SGHMC(model, \n",
    "                     subsample_positions=[0],\n",
    "                     batch_size=5, \n",
    "                     step_size=0.1, \n",
    "                     num_steps=4, \n",
    "                     with_friction=True, \n",
    "                     friction_term=None, \n",
    "                     friction_constant=1.0,\n",
    "                     obs_info_noise=False,\n",
    "                     do_mh_correction=False,\n",
    "                     do_step_size_adaptation=False,\n",
    "                     target_accept=0.75)\n",
    "\n",
    "# Use pyros MCMC sampling module\n",
    "sghmc_mcmc = MCMC(sghmc_kernel, num_samples=1000)\n",
    "# Pass the model parameters when we call run\n",
    "# Here we specify a stronger prior over the latent fairness of the coinflip\n",
    "sghmc_mcmc.run(data, alpha0=10., beta0=10.) # <------------------------\n",
    "# Alternatively we can specify neither alpha0 or beta0 and use their default values\n",
    "sghmc_mcmc.run(data)\n",
    "# Fetch samples from the sampler\n",
    "sghmc_samples = sghmc_mcmc.get_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9a230e",
   "metadata": {},
   "source": [
    "### Model with no positional arguments\n",
    "\n",
    "The following is a bad example where the observed data is passed as a key word argument, this will not be compatible with our implementation of SGHMC and you may get some undefined behaviour. So please pass any observed data that you wish to subsample as positional arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5160c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(data=torch.zeros(10)):\n",
    "    alpha0 = torch.tensor(1.0)\n",
    "    beta0 = torch.tensor(1.0)\n",
    "    \n",
    "    f = pyro.sample(\"coin\", dist.Beta(alpha0, beta0))\n",
    "    \n",
    "    return pyro.sample(\"obs\", dist.Bernoulli(f), obs=dats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cf3097",
   "metadata": {},
   "source": [
    "For example do this instead,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2606488c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.zeros(10)\n",
    "\n",
    "def model(data):\n",
    "    alpha0 = torch.tensor(1.0)\n",
    "    beta0 = torch.tensor(1.0)\n",
    "    \n",
    "    f = pyro.sample(\"coin\", dist.Beta(alpha0, beta0))\n",
    "    \n",
    "    return pyro.sample(\"obs\", dist.Bernoulli(f), obs=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedf7c3d",
   "metadata": {},
   "source": [
    "### Model with multiple positional arguments to subsample\n",
    "\n",
    "The below model specifies the outcome of two independent coin flips, we may wish to pass observation data as two seperate positional arguments. Our implementation of SGHMC is flexible enough to handle this use case we just need to set `subsample_positions` appropriately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8954ed60",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = torch.ones(10)\n",
    "x1[0:6] = torch.zeros(6)\n",
    "\n",
    "x2 = torch.ones(10)\n",
    "x2[0:2] = torch.zeros(2)\n",
    "\n",
    "def model(x1, x2, alpha0=1., beta0=1.):\n",
    "    alpha0 = torch.tensor(alpha0)\n",
    "    beta0 = torch.tensor(beta0)\n",
    "    \n",
    "    f1 = pyro.sample(\"coin1\", dist.Beta(alpha0, beta0))\n",
    "    f2 = pyro.sample(\"coin2\", dist.Beta(alpha0, beta0))\n",
    "    \n",
    "    return pyro.sample(\"obs1\", dist.Bernoulli(f1), obs=x1), pyro.sample(\"obs2\", dist.Bernoulli(f2), obs=x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d43362",
   "metadata": {},
   "source": [
    "We want to subsample both `x1` and `x2` and so we must set `subsample_positions=[0, 1]` given that `x1` and `x2` are positional argument `0` and `1` respectively. Otherwise, we could get some strange behaviour. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138b6166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the SGHMC kernel\n",
    "sghmc_kernel = SGHMC(model, \n",
    "                     subsample_positions=[0],\n",
    "                     batch_size=5, \n",
    "                     step_size=0.1, \n",
    "                     num_steps=4, \n",
    "                     with_friction=True, \n",
    "                     friction_term=None, \n",
    "                     friction_constant=1.0,\n",
    "                     obs_info_noise=False,\n",
    "                     do_mh_correction=False,\n",
    "                     do_step_size_adaptation=False,\n",
    "                     target_accept=0.75)\n",
    "\n",
    "# Use pyros MCMC sampling module\n",
    "sghmc_mcmc = MCMC(sghmc_kernel, num_samples=1000)\n",
    "# Pass the model parameters when we call run\n",
    "sghmc_mcmc.run(x1, x2)\n",
    "# Fetch samples from the sampler\n",
    "sghmc_samples = sghmc_mcmc.get_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8688c6",
   "metadata": {},
   "source": [
    "Note that it is **essential** that `x1` and `x2` be the same size, it doesn't make sense that during our trial of two independent coin flips we forget to flip one of the coins. It is always the case that when we condition on more than one positional argument the supplied data must have the same length in the first dimension. \n",
    "\n",
    "If you need more flexibility just pack the data into tensors and make sure the first dimensions are of equal size and unpack the data in the model. Although models that require this sort of flexbility can probably be refactored anyway to obey these restrictions with no semantic changes to the program."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bfcf94",
   "metadata": {},
   "source": [
    "### Silly example\n",
    "\n",
    "This silly example is just poor practice but it shows the flexbility of our SGHMC implementation. For example suppose `alpha0` is a positional argument and `beta0` is a key word argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13e5a126",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(data, alpha0, beta0=1.):\n",
    "    alpha0 = torch.tensor(alpha0)\n",
    "    beta0 = torch.tensor(beta0)\n",
    "    \n",
    "    f = pyro.sample(\"coin\", dist.Beta(alpha0, beta0))\n",
    "    \n",
    "    return pyro.sample(\"obs\", dist.Bernoulli(f), obs=dats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061eb0fb",
   "metadata": {},
   "source": [
    "Now when we run SGHMC we have to specify `alpha0` but we could just leave `beta0` as its default value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0b206e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the SGHMC kernel\n",
    "sghmc_kernel = SGHMC(model, \n",
    "                     subsample_positions=[0],\n",
    "                     batch_size=5, \n",
    "                     step_size=0.1, \n",
    "                     num_steps=4, \n",
    "                     with_friction=True, \n",
    "                     friction_term=None, \n",
    "                     friction_constant=1.0,\n",
    "                     obs_info_noise=False,\n",
    "                     do_mh_correction=False,\n",
    "                     do_step_size_adaptation=False,\n",
    "                     target_accept=0.75)\n",
    "\n",
    "# Use pyros MCMC sampling module\n",
    "sghmc_mcmc = MCMC(sghmc_kernel, num_samples=1000)\n",
    "# Pass the model parameters when we call run\n",
    "# alpha0 is positional argument 1 and we must specify it or we will get an error\n",
    "sghmc_mcmc.run(data, 1.)\n",
    "# We can specify beta0 too if we want but since it is a kwarg it is optional\n",
    "sghmc_mcmc.run(data, 10., beta0=10.)\n",
    "# Fetch samples from the sampler\n",
    "sghmc_samples = sghmc_mcmc.get_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c8e10b",
   "metadata": {},
   "source": [
    "The following instantiation of `sghmc_kernel` would be **wrong** since we don't want to subsample alpha0!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19253db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the SGHMC kernel\n",
    "sghmc_kernel = SGHMC(model, subsample_positions=[0, 1] ,batch_size=5, step_size=0.1, num_steps=4, with_friction=True, do_mh_correction=False)\n",
    "# This is worng since subsample_positions=[0, 1] which says we want to subsample positional argument 1, namely alpha0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
